{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0  Multiple Stream Input\n",
    "Multiple input video streams can be batched together through a single inference pipeline, producing an annotated, tiled output.  Any input format supported by GStreamer can be used for the input streams.  Any number of streams may be tiled together, up to the maximum number that the Jetson can handle - eight streams in the case of the Jetson Nano.  In this notebook, you'll work with the `deepstream-test3` reference application to build applications that use varying numbers of input streams and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/03_test3_example.png\" alt=\"beginning and end image with pipe\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.1 Build a Pipeline with Multiple Input Streams in Parallel](#4.1-Build-a-Pipeline-with-Multiple-Input-Streams-in-Parallel)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.1.1 Practice Application `deepstream-test3-rtsp_out`](#4.1.1-Practice-Application-deepstream-test3-rtsp_out)<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.1.2 Source Bins](#4.1.2-Source-Bins)<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.1.3 Putting the Multi-Stream Pipeline Together](#4.1.3-Putting-the-Multi-Stream-Pipeline-Together)<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.1.4 Exercise: Run the Base Application](#4.1.4-Exercise:-Run-the-Base-Application)<br>\n",
    "**[4.2 Configure Multiple Input Streams with Different Formats](#4.2-Configure-Multiple-Input-Streams-with-Different-Formats)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.2.1 GStreamer Autoplugging with `uridecodebin`](#4.2.1-GStreamer-Autoplugging-with-uridecodebin)<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.2.2 Batch Size Configuration](#4.2.2-Batch-Size-Configuration)<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.2.3 Exercise: Add an Input Source](#4.2.3-Exercise:-Add-an-Input-Source)<br>\n",
    "**[4.3 Put It All Together](#4.3-Put-It-All-Together)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[4.3.1 Exercise: Eight Input Streams](#4.3.1-Exercise:-Eight-Input-Streams)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Build a Pipeline with Multiple Input Streams in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application accepts one or more video streams as input.  You can use any GStreamer-supported file or streamed format.  Reference the stream using a **Uniform Resource Identifier (URI)** as input.  For example, a URI with syntax `file://</path/to/file>` is used for a video file.  \n",
    "\n",
    "This application includes a definition to create a **source bin**, which is basically a small decoder pipeline, for each input stream.  It then connects the source bins to the `Gst-nvstreammux` plugin.  This forms a **batch** of frames equal to the number of inputs. \n",
    "\n",
    "<img src=\"../../images/03_DS_plugin_gst-nvstreammux.png\">\n",
    "\n",
    "The batch of frames is fed to the `Gst-nvinfer` plugin for batched inferencing. A batched buffer is composited into a 2D tile array using the `Gst-nvmultistreamtiler` plugin. The rest of the pipeline is similar to the DeepStream pipeline used in the object detection exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Practice Application `deepstream-test3-rtsp_out` \n",
    "\n",
    "As with other reference applications in this course, a modified version of the reference app with RSTP output is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README\tdeepstream_test3_rtsp_out.py  dstest3_pgie_config.txt\n"
     ]
    }
   ],
   "source": [
    "# Set some path locations for readability\n",
    "PYTHON_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/apps'\n",
    "STREAMS = '/opt/nvidia/deepstream/deepstream/samples/streams'\n",
    "DLI_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/dli_apps'\n",
    "MY_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/my_apps'\n",
    "# Make sure common utilities are in my_apps\n",
    "!cp -r $PYTHON_APPS/common $MY_APPS/\n",
    "\n",
    "# List the contents of the sample app\n",
    "!ls $DLI_APPS/deepstream-test3-rtsp-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1.2 Source Bins\n",
    "The explicitly defined `GstFileSrc`, `GstH264Parse` and `Gst-nvv4l2decoder` elements we saw in `deepstream-test1` and `deepstream-test2` are not needed or used in this app.  Instead, this app uses the GStreamer `uridecoderbin` element to create the source and decoder elements dynamically, based on the input stream format. \n",
    "\n",
    "To see how this is put together, take a look at the `main()` code definition in  [deepstream-test3-rtsp-out/deepstream_test3_rtsp_out.py](deepstream/sources/deepstream_python_apps/dli_apps/deepstream-test3-rtsp-out/deepstream_test3_rtsp_out.py). *(Note: the code snippets below are abbreviated for clarity)*:\n",
    "\n",
    "\n",
    "Early in the code, the list of URI input paths (streams and files) are added to a Python list for processing:\n",
    "```python\n",
    "def main(args):\n",
    "\n",
    "    for i in range(0,len(args)-1):\n",
    "        fps_streams[\"stream{0}\".format(i)]=GETFPS(i)\n",
    "    number_sources=len(args)-1\n",
    "```\n",
    "\n",
    "The first element type in the pipeline to be created is `nvstreammux`, with name `streammux`, which is used to multiplex, or batch, the input sources.  For each URI path source provided by the user, a `source_bin` is created and added to the pipeline to be linked later.  Each source bin can be thought of as a mini-pipeline that automatically decodes the input referenced by the URI (either file or stream).  Each new source bin has its own source and sink pad:\n",
    "\n",
    "```python\n",
    "    # Create nvstreammux instance to form batches from one or more sources.\n",
    "    streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "    pipeline.add(streammux)\n",
    "\n",
    "    for i in range(number_sources):\n",
    "        print(\"Creating source_bin \",i,\" \\n \")\n",
    "        uri_name=args[i+1]\n",
    "        source_bin=create_source_bin(i, uri_name)\n",
    "        pipeline.add(source_bin)\n",
    "\n",
    "        padname=\"sink_%u\" %i\n",
    "        sinkpad= streammux.get_request_pad(padname) \n",
    "        srcpad=source_bin.get_static_pad(\"src\")\n",
    "        srcpad.link(sinkpad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1.3 Putting the Multi-Stream Pipeline Together\n",
    "To accommodate uneven streams and buffer fills, a series of GStreamer `queue` elements are added to the pipeline and linked between elements downstream from the inputs.\n",
    "To display multiple streams as one output, the `Gst-nvmultistreamtiler` element is required.  This element composites streams based on their stream-ids in row-major order (starting from stream 0, left to right across the top row, then across the next row, and so on).\n",
    "\n",
    "In summary, this DeepStream application uses the following elements for its pipeline:\n",
    "- `uridecoderbin` source bins - one or more instances created at runtime to read the video data\n",
    "- `Gst-nvstreammux` - batch video streams before sending for AI inference\n",
    "- `queue` - queue data until one of the limits specified by properties has been reached\n",
    "- `Gst-nvinfer` - runs inference using TensorRT\n",
    "- `queue` - queue data until one of the limits specified by properties has been reached\n",
    "- `Gst-nvmultistreamtiler` - plugin composites a 2D tile from batched buffers\n",
    "- `queue` - queue data until one of the limits specified by properties has been reached\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (I420 to RGBA)\n",
    "- `queue` - queue data until one of the limits specified by properties has been reached\n",
    "- `Gst-nvdsosd` - draw bounding boxes, text and region of interest (ROI) polygons\n",
    "- `queue` - queue data until one of the limits specified by properties has been reached\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (RGBA to I420)\n",
    "- `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "- `Gst-nvv4l2h264enc` - encodes RAW data in I420 format to H264\n",
    "- `GstRtpH264Pay` - converts H264 encoded Payload to RTP packets (RFC 3984)\n",
    "- `GstUDPSink` - sends UDP packets to the network. When paired with RTP payloader `Gst-rtph264pay` it can implement RTP streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4 Exercise: Run the Base Application\n",
    "For this exercise, we will use video files provided in the DeepStream SDK Samples directory for our streams.  This is for convenience and demonstration purposes.  Any of the large number of formats that GStreamer `uridecoderbin` supports will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_1080p_h264.mp4  sample_720p.mp4\t      sample_qHD.mp4\t     yoga.jpg\n",
      "sample_1080p_h265.mp4  sample_cam6.mp4\t      sample_ride_bike.mov   yoga.mp4\n",
      "sample_720p.h264       sample_industrial.jpg  sample_run.mov\n",
      "sample_720p.jpg        sample_push.mov\t      sample_walk.mov\n",
      "sample_720p.mjpeg      sample_qHD.h264\t      sonyc_mixed_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# List the sample video streams available\n",
    "!ls $STREAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: deepstream_test3_rtsp_out.py [-h] -i INPUT [INPUT ...] [-c {H264,H265}]\n",
      "                                    [-b BITRATE]\n",
      "\n",
      "RTSP Output Sample Application Help\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i INPUT [INPUT ...], --input INPUT [INPUT ...]\n",
      "                        List of Paths to input H264 elementry streams\n",
      "  -c {H264,H265}, --codec {H264,H265}\n",
      "                        RTSP Streaming Codec H264/H265 , default=H264\n",
      "  -b BITRATE, --bitrate BITRATE\n",
      "                        Set the encoding bitrate\n"
     ]
    }
   ],
   "source": [
    "# Check usage of the test3 app with the help option\n",
    "!cd $DLI_APPS/deepstream-test3-rtsp-out \\\n",
    "    && python3 deepstream_test3_rtsp_out.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the DeepStream app\n",
    "If using VLC media player, open the app on your computer:\n",
    "- Pull down the \"Media\" menu and select the \"Open Network Stream\" dialog.\n",
    "- Set the URL to `rtsp://192.168.55.1:8554/ds-test`.\n",
    "- Optionally, add a wait delay to VLC:\n",
    "   - Click \"Show more options\" in the dialog.\n",
    "   - Add ` :ipv4=120000` to the \"Edit Options\" line to add a 120 second delay.\n",
    "- Start execution of the cell below.\n",
    "- Click \"Play\" on your VLC media player *after* you start the cell execution.  \n",
    "\n",
    "The stream will start from the Jetson Nano and display in the media player.  There is a delay while the model `.engine` file is built.  \n",
    "\n",
    "If VLC fails, start it again. Close the VLC fail notice and press the \"play\" triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pipeline \n",
      " \n",
      "Creating streamux \n",
      " \n",
      "Creating source_bin  0  \n",
      " \n",
      "Creating source bin\n",
      "source-bin-00\n",
      "Creating source_bin  1  \n",
      " \n",
      "Creating source bin\n",
      "source-bin-01\n",
      "Creating Pgie \n",
      " \n",
      "Creating tiler \n",
      " \n",
      "Creating nvvidconv \n",
      " \n",
      "Creating nvosd \n",
      " \n",
      "Creating H264 Encoder\n",
      "Creating H264 rtppay\n",
      "WARNING: Overriding infer-config batch-size 1  with number of sources  2  \n",
      "\n",
      "Adding elements to Pipeline \n",
      "\n",
      "Linking elements in the Pipeline \n",
      "\n",
      "\n",
      " *** DeepStream: Launched RTSP Streaming at rtsp://localhost:8554/ds-test ***\n",
      "\n",
      "\n",
      "Now playing...\n",
      "1 :  file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264\n",
      "2 :  file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4\n",
      "Starting pipeline \n",
      "\n",
      "Opening in BLOCKING MODE \n",
      "0:00:00.355697932 \u001b[332m  655\u001b[00m     0x2c854a60 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:635:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::initialize() <nvdsinfer_context_impl.cpp:1161> [UID = 1]: Warning, OpenCV has been deprecated. Using NMS for clustering instead of cv::groupRectangles with topK = 20 and NMS Threshold = 0.5\n",
      "0:00:06.189355400 \u001b[332m  655\u001b[00m     0x2c854a60 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:1900> [UID = 1]: deserialized trt engine from :/opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b1_gpu0_fp16.engine\n",
      "INFO: [Implicit Engine Info]: layers num: 3\n",
      "0   INPUT  kFLOAT input_1         3x368x640       \n",
      "1   OUTPUT kFLOAT conv2d_bbox     16x23x40        \n",
      "2   OUTPUT kFLOAT conv2d_cov/Sigmoid 4x23x40         \n",
      "\n",
      "0:00:06.190512544 \u001b[332m  655\u001b[00m     0x2c854a60 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:635:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::checkBackendParams() <nvdsinfer_context_impl.cpp:1833> [UID = 1]: Backend has maxBatchSize 1 whereas 2 has been requested\n",
      "0:00:06.190565513 \u001b[332m  655\u001b[00m     0x2c854a60 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:635:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2012> [UID = 1]: deserialized backend context :/opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b1_gpu0_fp16.engine failed to match config params, trying rebuild\n",
      "0:00:06.194669345 \u001b[332m  655\u001b[00m     0x2c854a60 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() <nvdsinfer_context_impl.cpp:1914> [UID = 1]: Trying to create engine from model files\n",
      "WARNING: INT8 not supported by platform. Trying FP16 mode.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Run the app with two input streams\n",
    "!cd $DLI_APPS/deepstream-test3-rtsp-out \\\n",
    "    && python3 deepstream_test3_rtsp_out.py -i \\\n",
    "        file://$STREAMS/sample_720p.h264 \\\n",
    "        file://$STREAMS/sample_720p.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Configure Multiple Input Streams with Different Formats\n",
    "Input streams with different formats are automatically accommodated with matching decoders at runtime using **autoplugging**.  There is _no configuration necessary_ when changing _formats_ for this app!\n",
    "\n",
    "However, when changing _the number of inputs (batch size)_, configuration is recommended for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2.1 GStreamer Autoplugging with `uridecodebin`\n",
    "\n",
    "Each input stream is sourced and configured with the aid of the `uridecodebin` GStreamer element at runtime.  This does lengthen the startup time for the application, but also offers robust flexibility. A `for` loop in `main()` creates a source bin for each URI input by the user on the command line.\n",
    "\n",
    "To create the source bins, the `create_source_bin()` function is called.  This function creates a `uridecodebin` element for each input URI path. *(Note: the sample snippets below are abbreviated code for clarity purposes)* :\n",
    "\n",
    "```python\n",
    "def create_source_bin(index,uri):\n",
    "    print(\"Creating source bin\")\n",
    "\n",
    "    # Create a source GstBin to abstract this bin's content from the rest of the\n",
    "    # pipeline\n",
    "    bin_name=\"source-bin-%02d\" %index\n",
    "    nbin=Gst.Bin.new(bin_name)\n",
    "\n",
    "    # Source element for reading from the uri.\n",
    "    # We will use decodebin and let it figure out the container format of the\n",
    "    # stream and the codec and plug the appropriate demux and decode plugins.\n",
    "    uri_decode_bin=Gst.ElementFactory.make(\"uridecodebin\", \"uri-decode-bin\")\n",
    "\n",
    "    # We set the input uri to the source element\n",
    "    uri_decode_bin.set_property(\"uri\",uri)\n",
    "    # Connect to the \"pad-added\" signal of the decodebin which generates a\n",
    "    # callback once a new pad for raw data has beed created by the decodebin\n",
    "    uri_decode_bin.connect(\"pad-added\",cb_newpad,nbin)\n",
    "    uri_decode_bin.connect(\"child-added\",decodebin_child_added,nbin)\n",
    "\n",
    "    # We need to create a ghost pad for the source bin which will act as a proxy\n",
    "    # for the video decoder src pad. The ghost pad will not have a target right\n",
    "    # now. Once the decode bin creates the video decoder and generates the\n",
    "    # cb_newpad callback, we will set the ghost pad target to the video decoder\n",
    "    # src pad.\n",
    "    Gst.Bin.add(nbin,uri_decode_bin)\n",
    "    bin_pad=nbin.add_pad(Gst.GhostPad.new_no_target(\"src\",Gst.PadDirection.SRC))\n",
    "    return nbin\n",
    "```\n",
    "\n",
    "The element is instantiated as a `uridecodebin` element. This element adds and links \"child\" decoder plugin elements to the source bin as needed, based on the actual input stream formatting.  Finally, a source pad is added to the new mini-pipeline so that it can later be easily linked to the `Gst-nvstreammux` plugin element and the rest of the main pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2.2 Batch Size Configuration\n",
    "The configuration file includes two parameters related to the number of input streams, or batch size, for the application:\n",
    "- `batch-size` - number of input streams to be batched in parallel\n",
    "- `model-engine-file` - path to the `.engine` file which is built for a specific batch size\n",
    "\n",
    "If the `batch-size` does not match the number of input streams specified at runtime, the app will override the value with the correct number.\n",
    "\n",
    "The `.engine` file is built at runtime if it does not already exist.  If an engine file with the wrong batch size is provided in the configuration file, a warning will appear, and the correct `.engine` will be built and stored, prior to running the streams.  This adds time to the start of the run and can be avoided, at least after the first run, by including the correct path for the `.engine`.  Here's some example output with a batch size mismatch:\n",
    "\n",
    "```c\n",
    "0:00:06.165375005  6726   0x5592aa1380 WARN                 nvinfer gstnvinfer.cpp:515:gst_nvinfer_logger:<primary-nvinference-engine> NvDsInferContext[UID 1]:checkEngineParams(): Requested Max Batch Size is less than engine batch size\n",
    "0:00:06.166695340  6726   0x5592aa1380 INFO                 nvinfer gstnvinfer.cpp:519:gst_nvinfer_logger:<primary-nvinference-engine> NvDsInferContext[UID 1]:initialize(): Trying to create engine from model files\n",
    "0:01:42.046291797  6726   0x5592aa1380 INFO                 nvinfer gstnvinfer.cpp:519:gst_nvinfer_logger:<primary-nvinference-engine> NvDsInferContext[UID 1]:generateTRTModel(): Storing the serialized cuda engine to file at /home/dlinano/deepstream_sdk_v4.0.2_jetson/samples/models/Primary_Detector/resnet10.caffemodel_b3_fp16.engine\n",
    "```\n",
    "\n",
    "The batch size is coded in the name of the `.engine` file created.  In the preceding example, the engine file for three input streams was stored as `resnet10.caffemodel_b3_fp16.engine`, whereas the name of the  engine with two input streams is `resnet10.caffemodel_b2_fp16.engine`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 Exercise: Add an Input Source\n",
    "Create a new app based on `deepstream-test3-rstp-out` that can accept and tile three video input streams.  The application itself will not need to be modified, but the configuration file will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new app located at $MY_APPS/dst3-three-streams \n",
    "#      based on $DLI_APPS/deepstream-test3-rtsp-out\n",
    "!mkdir -p $MY_APPS/dst3-three-streams\n",
    "!cp -rfv $DLI_APPS/deepstream-test3-rtsp-out/* $MY_APPS/dst3-three-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the parameters for `batch-size` and `model-engine-file` in the [configuration file](my_apps/dst3-three-streams/dstest3_pgie_config.txt) of your new app, to expect three input streams.  Then build and run the app to see if it worked!\n",
    "\n",
    "*Note: The first time you build this app, there will be a delay while the correct `.engine` file is built and stored.*   **This can take from one to five minutes to build.** . *To avoid timeouts in the media player during this wait, don't start the media player on your computer until the `.engine` build is complete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the app with three input streams\n",
    "!cd $MY_APPS/dst3-three-streams \\\n",
    "    && python3 deepstream_test3_rtsp_out.py -i \\\n",
    "        file://$STREAMS/sample_720p.h264 \\\n",
    "        file://$STREAMS/sample_720p.mp4  \\\n",
    "        file://$STREAMS/sample_1080p_h265.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 deepstream_test3_rtsp_out.py -i \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.h264 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.mp4 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.h264 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.mp4 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.h264 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.mp4 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.h264 \\\n",
    "    file:///dli/task/deepstream_apps/sample_720p.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "If you see something like this image, you did it!  If not, keep trying, or take a peek at the [configuration solution](solutions/ex4.2.3_ThreeStreams/dstest3_pgie_config.txt) in the solutions directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/03_three_streams.png\" alt=\"three streams\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Put It All Together\n",
    "Great job fixing the config file to add another input stream!  Now, push Jetson Nano to it's limit by running the maximum number of input streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Exercise: Eight Input Streams\n",
    "Create a new app based on `deepstream-test3-rstp-out` that can accept and tile eight video input streams. Fill in the following cells with appropriate commands to create,and run your app. To edit your files, use the JupyterLab file browser at left to navigate to the correct folder; then, double click on the file you wish to open and edit.\n",
    "\n",
    "*Notes:* \n",
    "* *The first time you build this app, there will be a delay while the correct `.engine` file is built and stored.*   **This can take up to five minutes to build.**  *To avoid timeouts in the media player during this wait, don't start the media player on your computer until the `.engine` build is complete.*\n",
    "* *Performance may be degraded with eight streams over the RSTP output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Create a new app located at $MY_APPS/dst3-eight-streams \n",
    "#      based on $DLI_APPS/deepstream-test3-rtsp-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the PGIE configuration file to process a batch size of eight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Run the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "If you see something like this image, you've mastered multiple input streams!  If not, keep trying, or take a peek at the [solution code](solutions/ex4.3.1_EightStreams/solution-4.3.1.ipynb) in the solutions directory.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/03_8streams.png\" alt=\"8 streams\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You configured a DeepStream pipeline to accept different input streams and are able to apply inference on those streams.<br>\n",
    "Move on to [5.0 Video File Output](./05_VideoFileOutput.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
