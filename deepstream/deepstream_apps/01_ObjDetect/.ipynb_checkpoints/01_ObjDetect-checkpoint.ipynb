{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Object Detection Application\n",
    "In this notebook, you'll work with the `deepstream-test1` reference application to find objects in a video stream, annotate them with bounding boxes, and output the annotated stream along with a count of the objects found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_threethingsio.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll follow the steps below to build your own applications based on the reference app:\n",
    "\n",
    "**[1.1 Build a Basic DeepStream Pipeline](#1.1-Build-a-Basic-DeepStream-Pipeline)**<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.1.1 Sample Application - `deepstream-test1`](#1.1.1-Sample-Application---deepstream-test1)<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.1.2 Sample Application Plus RTSP - `deepstream-test1-rtsp-out`](#1.1.2-Sample-Application-Plus-RTSP---deepstream-test1-rtsp-out)<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.1.3 Putting the Pipeline Together](#1.1.3-Putting-the-Pipeline-Together)<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.1.4 Exercise: Run the Base Application](#1.1.4-Exercise:-Run-the-Base-Application)<br>\n",
    "**[1.2 Configure an Object Detection Model](#1.2-Configure-an-Object-Detection-Model)**<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.2.1 `Gst-nvinfer` Configuration File](#1.2.1-Gst-nvinfer-Configuration-File)<br>\n",
    "&nbsp; &nbsp; &nbsp; [1.2.2 Exercise: Detect Only Two Object Types](#1.2.2-Exercise:-Detect-Only-Two-Object-Types)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Build a Basic DeepStream Pipeline\n",
    "The framework used to build a DeepStream application is a GStreamer **pipeline** consisting of a video input stream, a series of **elements** or **plugins** to process the stream, and an insightful output stream.  Plugins along the pipeline are sometimes referred to as **filters**, because they have both an input, also called the **sink**, and an output, called the **source**. The plugins at the start and end of the pipeline only have a source or sink and are referred to generally as source or sink plugins.\n",
    "\n",
    "In the pipeline, the source **pad** of one plugin connects to the sink pad of the next in line.  The source includes data extracted from the processing, the **metadata**, which can be used for annotation of the video and other insights about the input stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_building_blocks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1.1 Sample Application - `deepstream-test1`\n",
    "The DeepStream SDK includes plugins for building a pipeline, and reference applications. For example, the `deepstream_test1` reference application can take a street scene video file as input, use object detection to find vehicles, people, bicycles, and road signs within the video, and output a video stream with bounding boxes around the objects found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_exampleio2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference test applications are in the `sources` folder of the DeepStream SDK, which is located at `/opt/nvidia/deepstream/deepstream`.  This is linked in your workspace as simply `deepstream`. \n",
    "\n",
    "You can open and look at the Python code for the `deepstream-test1` app at [deepstream/sources/deepstream_python_apps/apps/deepstream-test1/deepstream_test_1.py](deepstream/sources/deepstream_python_apps/apps/deepstream-test1/deepstream_test_1.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code, we can find where all the plugins are instantiated in `main()` definition using the `Gst.ElementFactory.make()` method.  This is a good way to see exactly which plugins are in the pipeline *(Note: the sample snippets shown are abbreviated code for clarity purposes)*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Create gstreamer elements\n",
    "    # Create Pipeline element that will form a connection of other elements\n",
    "    print(\"Creating Pipeline \\n \")\n",
    "    pipeline = Gst.Pipeline()\n",
    "\n",
    "    # Source element for reading from the file\n",
    "    print(\"Creating Source \\n \")\n",
    "    source = Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "\n",
    "    # Since the data format in the input file is elementary h264 stream,\n",
    "    # we need a h264parser\n",
    "    print(\"Creating H264Parser \\n\")\n",
    "    h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "\n",
    "    # Use nvdec_h264 for hardware accelerated decode on GPU\n",
    "    print(\"Creating Decoder \\n\")\n",
    "    decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "\n",
    "    # Create nvstreammux instance to form batches from one or more sources.\n",
    "    streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "\n",
    "    # Use nvinfer to run inferencing on decoder's output,\n",
    "    # behaviour of inferencing is set through config file\n",
    "    pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "\n",
    "    # Use convertor to convert from NV12 to RGBA as required by nvosd\n",
    "    nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n",
    "\n",
    "    # Create OSD to draw on the converted RGBA buffer\n",
    "    nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "\n",
    "    # Finally render the osd output\n",
    "    if is_aarch64():\n",
    "        transform = Gst.ElementFactory.make(\"nvegltransform\", \"nvegl-transform\")\n",
    "\n",
    "    sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the input is a file source, `filesrc`, in H.264 video format.  It is parsed (`h264parse`), decoded (`nvv4l2decoder`), batched (`nvstreammux`), and then run through the `nvinfer` inference engine to detect objects.  A buffer is created with `nvvideoconvert` so that bounding boxes can be overlaid on the video images with the `nvdsosd` plugin.  Finally, the output is rendered, in this case for a display using `nvegltransform` and `nvegglessink`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1.2 Sample Application Plus RTSP - `deepstream-test1-rtsp-out`\n",
    "For the purposes of this lab, which runs headless on a Jetson Nano connected to a laptop, the video stream must be converted to a format that can be transferred to the laptop media player.  This is accomplished with additional plugins and some logic for the rendering portion of the pipeline. Review the code in [deepstream/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/deepstream_test1_rtsp_out.py](deepstream/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/deepstream_test1_rtsp_out.py).<br><br>\n",
    "Scrolling down to `main()`, we can see that there are a few differences in the rendering plugins after the OSD (On Screen Display) creation.  Instead of using the video sink, the stream is filtered, encoded, formatted for RTP payloads, and finally sinked as UDP packets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    # Create OSD to draw on the converted RGBA buffer\n",
    "    nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "    nvvidconv_postosd = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor_postosd\")\n",
    "\n",
    "    # Create a caps filter\n",
    "    caps = Gst.ElementFactory.make(\"capsfilter\", \"filter\")\n",
    "    \n",
    "    # Make the encoder\n",
    "    if codec == \"H264\":\n",
    "        encoder = Gst.ElementFactory.make(\"nvv4l2h264enc\", \"encoder\")\n",
    "    elif codec == \"H265\":\n",
    "        encoder = Gst.ElementFactory.make(\"nvv4l2h265enc\", \"encoder\")\n",
    "    \n",
    "    # Make the payload-encode video into RTP packets\n",
    "    if codec == \"H264\":\n",
    "        rtppay = Gst.ElementFactory.make(\"rtph264pay\", \"rtppay\")\n",
    "    elif codec == \"H265\":\n",
    "        rtppay = Gst.ElementFactory.make(\"rtph265pay\", \"rtppay\")\n",
    "\n",
    "    # Make the UDP sink\n",
    "    updsink_port_num = 5400\n",
    "    sink = Gst.ElementFactory.make(\"udpsink\", \"udpsink\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1.3 Putting the Pipeline Together\n",
    "The plugins are put in a pipeline with the `pipeline.add()` method:\n",
    "\n",
    "```python\n",
    "    pipeline.add(source)\n",
    "    pipeline.add(h264parser)\n",
    "    pipeline.add(decoder)\n",
    "    pipeline.add(streammux)\n",
    "    pipeline.add(pgie)\n",
    "    pipeline.add(nvvidconv)\n",
    "    pipeline.add(nvosd)\n",
    "    pipeline.add(nvvidconv_postosd)\n",
    "    pipeline.add(caps)\n",
    "    pipeline.add(encoder)\n",
    "    pipeline.add(rtppay)\n",
    "    pipeline.add(sink)\n",
    "```\n",
    "\n",
    "Each plugin is then connected in order using its `.link()` method. Generally, this is as simple as \n",
    "\n",
    "```python\n",
    "    source_plugin.link(sink_plugin)\n",
    "```\n",
    "\n",
    "However, when connecting a source to nvstreammux (`streammux` or \"the muxer\"), a new sink pad must be requested from the muxer, and that pad explicitly linked to the previous plugin's source pad.  In the code, there is a `srcpad` into `streammux` and a `srcpad` out of `decoder` defined.  These are then linked together directly. \n",
    "\n",
    "```python\n",
    "    source.link(h264parser)\n",
    "    h264parser.link(decoder)\n",
    "    sinkpad = streammux.get_request_pad(\"sink_0\")  \n",
    "    srcpad = decoder.get_static_pad(\"src\")\n",
    "    srcpad.link(sinkpad)\n",
    "    streammux.link(pgie)\n",
    "    pgie.link(nvvidconv)\n",
    "    nvvidconv.link(nvosd)\n",
    "    nvosd.link(nvvidconv_postosd)\n",
    "    nvvidconv_postosd.link(caps)\n",
    "    caps.link(encoder)\n",
    "    encoder.link(rtppay)\n",
    "    rtppay.link(sink)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the pipeline for this app consists of the following plugins (ordered):\n",
    "\n",
    "- `GstFileSrc` - reads the video data from file\n",
    "- `GstH264Parse` - parses the incoming H264 stream\n",
    "- `Gst-nvv4l2decoder` - hardware accelerated decoder; decodes video streams using NVDEC\n",
    "- `Gst-nvstreammux` - batch video streams before sending for AI inference\n",
    "- `Gst-nvinfer` - runs inference using TensorRT\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (I420 to RGBA)\n",
    "- `Gst-nvdsosd` - draw bounding boxes, text and region of interest (ROI) polygons\n",
    "- `Gst-nvvideoconvert` - performs video color format conversion (RGBA to I420)\n",
    "- `GstCapsFilter` - enforces limitations on data (no data modification)\n",
    "- `Gst-nvv4l2h264enc` - encodes RAW data in I420 format to H264\n",
    "- `GstRtpH264Pay` - converts H264 encoded Payload to RTP packets (RFC 3984)\n",
    "- `GstUDPSink` - sends UDP packets to the network. When paired with RTP payloader (`Gst-rtph264pay`) it can implement RTP streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Exercise: Run the Base Application\n",
    "In this exercise, we'll feed a simple video file through the pipeline and view the result using an RTSP stream.  \n",
    "\n",
    "In the `deepstream-test1`/`deepstream-test1-rtsp-out` example app, object detection is performed on a per-frame basis. Counts for `Vehicle` and `Person` objects are also tracked.  Bounding boxes are drawn around the objects identified, and a counter display is overlaid in the upper left corner of the video. \n",
    "\n",
    "To begin, assign some user-friendly names to paths.  Next, list the available sample apps and video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some path locations for readability\n",
    "PYTHON_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/apps'\n",
    "STREAMS = '/opt/nvidia/deepstream/deepstream/samples/streams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the sample Python apps available\n",
    "!ls $PYTHON_APPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the sample video streams available\n",
    "!ls $STREAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the app, check out the script usage with the `--help` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check usage of the test1 app with the help option\n",
    "!cd $PYTHON_APPS/deepstream-test1-rtsp-out \\\n",
    "    && python3 deepstream_test1_rtsp_out.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the DeepStream app\n",
    "If using VLC media player, open the app on your computer:\n",
    "- Pull down the \"Media\" menu and select the \"Open Network Stream\" dialog.\n",
    "- Set the URL to `rtsp://192.168.55.1:8554/ds-test`.\n",
    "- Optionally, add a wait delay to VLC:\n",
    "   - Click \"Show more options\" in the dialog.\n",
    "   - Add ` :ipv4=120000` to the \"Edit Options\" line to add a 120 second delay.\n",
    "- Start execution of the cell below.\n",
    "- Click \"Play\" on your VLC media player *after* you start the cell execution.  \n",
    "\n",
    "The stream will start from the Jetson Nano and display in the media player.  There is a delay while the model `.engine` file is built.  \n",
    "\n",
    "If VLC fails, start it again. Close the VLC fail notice and press the \"play\" triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the app\n",
    "!cd $PYTHON_APPS/deepstream-test1-rtsp-out \\\n",
    "    && python3 deepstream_test1_rtsp_out.py -i $STREAMS/sample_720p.h264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Configure an Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample application shows counts for two types of objects: `Vehicle` and `Person`.  This is specified in the display output line (line 101):\n",
    "\n",
    "```python\n",
    "py_nvosd_text_params.display_text = \n",
    "    \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\" \\\n",
    "    .format(frame_number, \n",
    "            num_rects, \n",
    "            obj_counter[PGIE_CLASS_ID_VEHICLE], \n",
    "            obj_counter[PGIE_CLASS_ID_PERSON])\n",
    "```\n",
    "\n",
    "However, the model used can actually detect four types of objects as revealed in the class ID assignments in the application script:\n",
    "\n",
    "```python\n",
    "PGIE_CLASS_ID_VEHICLE = 0\n",
    "PGIE_CLASS_ID_BICYCLE = 1\n",
    "PGIE_CLASS_ID_PERSON = 2\n",
    "PGIE_CLASS_ID_ROADSIGN = 3\n",
    "```\n",
    "\n",
    "If you watch the application stream carefully, there is a bicycle in the early frames.  It is detected and boxed briefly.  The same is true for road signs.  You can see this when using a different stream such as the `sample_qHD.h264` sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 `Gst-nvinfer` Configuration File\n",
    "The classification labels (the types of objects detected) are specific to the model used for the inference, which in this case is a sample model provided with the DeepStream SDK.  The `Gst-nvinfer` plugin employs a configuration file to specify the model and various properties. Open the configuration file for the app we are using at [deepstream/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/dstest1_pgie_config.txt](deepstream/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/dstest1_pgie_config.txt).  The `Gst-nvinfer` configuration file uses a key file format, with details on key names found in the [DeepStream Developer Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html#gst-nvinfer-file-configuration-specifications).\n",
    "- The **\\[property\\]** group configures the general behavior of the plugin. It is the only mandatory group.\n",
    "- The **\\[class-attrs-all\\]** group configures detection parameters for all classes.\n",
    "- The **\\[class-attrs-\\<class-id\\>\\]** group configures detection parameters for a class specified by \\<class-id\\>. For example, the \\[class-attrs-2\\] group configures detection parameters for class ID 2\\. This type of group has the same keys as \\[class-attrs-all\\]. \n",
    "\n",
    "For the most part, we can use the default values.  There are a few improvements we can make, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [property] : `model-engine-file`\n",
    "During the previous test run, you may have noticed that an error was produced:\n",
    "\n",
    "```C\n",
    "ERROR: Deserialize engine failed because file path: /opt/nvidia/deepstream/deepstream-6.0/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/../../../../samples/models/Primary_Detector/resnet10.caffemodel_b1_gpu0_int8.engine open error\n",
    "```\n",
    "\n",
    "This error indicates that the TensoRT optimized model engine is not present.  Since it is not there, an attempt is made to build it.  There is a problem though...\n",
    "\n",
    "```c\n",
    "WARNING: INT8 not supported by platform. Trying FP16 mode.\n",
    "```\n",
    "\n",
    "The Jetson Nano does not support INT8 mode, so an FP16 engine is built instead.  When it's complete, inference is run on the file stream and we see the result in the RTSP output.  Along the way, there is a notice that the engine was created and where it resides:\n",
    "\n",
    "```c\n",
    "serialize cuda engine to file: /opt/nvidia/deepstream/deepstream-6.0/samples/models/Primary_Detector/resnet10.caffemodel_b1_gpu0_fp16.engine successfully\n",
    "```\n",
    "\n",
    "This engine now exists and can be reused, which saves a lot of time if you want to run the app again.  Unfortunately, since the configuration file specifies a different engine (the INT8 engine), the engine will be rebuilt anyway, which will cause an unnecessary delay!  \n",
    "\n",
    "To reuse the engine just built, the configuration property, `model-engine-file`, must be set to the correct path.  The next cell provides a quick substitution fix.  Go ahead and execute it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the engine to fp16\n",
    "!sed -i 's/_int8.engine/_fp16.engine/g' $PYTHON_APPS/deepstream-test1-rtsp-out/dstest1_pgie_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [class-attrs-\\<all or class-id\\>] : `pre-cluster-threshold`\n",
    "The number of classes and the ordered `labels.txt` file path are specified in the \\[property\\] group along with the model engine. To configure which of these labels the object detector actually recognizes, we can change keys in the \\[class-attrs-all\\] and \\[class-attrs-\\<class-id\\>\\] groups.  The initial sample configuration file includes the following:\n",
    "```c\n",
    "[class-attrs-all]\n",
    "pre-cluster-threshold=0.2\n",
    "eps=0.2\n",
    "group-threshold=1\n",
    "```\n",
    "\n",
    "The `pre-cluster-threshold=0.2` key sets the detection confidence score. This tells us that all objects with a 20% confidence score or better will be marked as detected. If the threshold is greater than 1.0, then no objects will be detected, because a confidence of more than 100% would be required which is impossible!  \n",
    "\n",
    "This \"all\" grouping is not granular enough if we only want to detect a subset of the objects possible, or if we wish to use a different confidence level with different objects.  For example, we might choose to detect only vehicles, or to identify people with a different confidence level than road signs.  To specify a threshold for the four individual objects available in this model, we can add a specific group to the config file for each class: \n",
    "\n",
    "- \\[class-attrs-0\\] for vehicles\n",
    "- \\[class-attrs-1\\] for bicycles\n",
    "- \\[class-attrs-2\\] for persons\n",
    "- \\[class-attrs-3\\] for road signs\n",
    "\n",
    "Then, in each group, we can specify the threshold value.  This can be used to determine object detection for each of the four object categories individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Exercise: Detect Only Two Object Types\n",
    "Create a new app based on `deepstream-test1-rtsp_out` that detects *only* cars and bicycles. Begin by copying the existing app to a new workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the workspace for my new Python apps\n",
    "MY_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/my_apps'\n",
    "!cp -r $PYTHON_APPS/common $MY_APPS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new app located at my_apps/dst1-two-objects \n",
    "#      based on deepstream-test1-rtsp_out\n",
    "!mkdir -p $MY_APPS/dst1-two-objects\n",
    "!cp -rfv $PYTHON_APPS/deepstream-test1-rtsp-out/* $MY_APPS/dst1-two-objects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you just learned, modify the [configuration file](deepstream/sources/deepstream_python_apps/my_apps/dst1-two-objects/dstest1_pgie_config.txt) in your new app to detect *only* cars and bicycles.  You will need to add *class-specific groups* for each of the four classes to the end of your configuration file.<br>\n",
    "Class-specific example:\n",
    "   ```\n",
    "    # Per class configuration\n",
    "    # car\n",
    "    [class-attrs-0] \n",
    "    pre-cluster-threshold=0.2\n",
    "   ```\n",
    "Then, run the app to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the app\n",
    "!cd $MY_APPS/dst1-two-objects \\\n",
    "    && python3 deepstream_test1_rtsp_out.py -i $STREAMS/sample_720p.h264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "If you see something like this image, with only bicycles and cars detected you did it!  If not, keep trying or take a peek at the [solution](solutions/ex1.2.2_DetectTwo/ex1.2.2_dstest1_pgie_config.txt) config file in the solutions directory. If you aren't satisfied with the detection of the bicycle, you can experiment with the confidence threshold value. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_bikes_and_cars.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've run your first DeepStream sample app and created a new DeepStream app to detect different objects in a scene.<br>\n",
    "Move on to [2.0 Analysis with Metadata](02_Metadata.ipynb) to expand your video analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
