{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.0 Analysis with Metadata\n",
    "In this lab, you'll modify the video analytics in the basic object detection application you worked with .  Using a GStreamer probe, you'll extract and modify metadata to analyze the stream and annotate the display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_threethingsio.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll follow the steps below to build your own applications based on the reference app:\n",
    "\n",
    "**[2.1 Access Metadata to Perform Analysis](#2.1-Access-Metadata-to-Perform-Analysis)**<br>\n",
    "&nbsp; &nbsp; &nbsp; [2.1.1 Extract Metadata with a GStreamer Probe](#2.1.1-Extract-Metadata-with-a-GStreamer-Probe)<br>\n",
    "&nbsp; &nbsp; &nbsp; [2.1.2 Display Frame Information through MetaData](#2.1.2-Display-Frame-Information-through-MetaData)<br>\n",
    "&nbsp; &nbsp; &nbsp; [2.1.3 Object Text Display](#2.1.3-Object-Text-Display)<br>\n",
    "&nbsp; &nbsp; &nbsp; [2.1.4 Exercise: Count Vehicles and Bikes](#2.1.4-Exercise:-Count-Vehicles-and-Bikes)<br>\n",
    "**[2.2 Put It All Together](#2.2-Put-It-All-Together)**<br>\n",
    "&nbsp; &nbsp; &nbsp; [2.2.1 Exercise: Detect and Count Three Object Types](#2.2.1-Exercise:-Detect-and-Count-Three-Object-Types)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Access Metadata to Perform Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, you completed an exercise that modified the config file so that only the vehicles and bicycles are detected.  However, the application displays a count of the `Person` and `Vehicle` objects, which are not the objects we are detecting!  For this application, we would like to show the counts for the bicycles instead of people.  \n",
    "\n",
    "The `Gst-nvinfer` plugin finds objects and provides metadata about them as an output on its source pad, which is passed along through the pipeline within the metadata structure.  Using a GStreamer **probe** function, we can extract this metadata from the pipeline farther along the stream, count the objects detected, and update the screen overlay to display the information we want to highlight. \n",
    "\n",
    "The probe must be attached to a pad on a plugin. In this application, the probe is attached to the sink pad (the input) of the `Gst-nvdsosd` plugin with the following Python line in the app:\n",
    "\n",
    "```python\n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_test1_app.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Extract Metadata with a GStreamer Probe\n",
    "The `osd_sink_pad_buffer_probe()` function in the [deepstream-test1_rtsp_out](deepstream/sources/deepstream_python_apps/apps/deepstream-test1-rtsp-out/deepstream_test1_rtsp_out.py) app is a callback that executes each time there is new frame data on the sink pad. With this probe, we can extract a snapshot of the metadata coming into the `Gst-nvdsosd` plugin, and count the current objects.  The metadata structure is shown in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "    <img src=\"images/DS_plugin_metadata_720.png\"\n",
    "         alt=\"metadata\">\n",
    "    <figcaption style=\"text-align:center;\"><b>Metadata Structure Diagram</b></figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "After some variable initialization, the probe function collects the DeepStream metadata (structure `NvDsBatchMeta`) from the GStreamer buffer as `batch_meta` and further identifies the `frame_meta_list` element as `l_frame`:\n",
    "\n",
    "```python\n",
    "    gst_buffer = info.get_buffer()\n",
    "    if not gst_buffer:\n",
    "        print(\"Unable to get GstBuffer \")\n",
    "        return\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n",
    "    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _while_ frame loop identifies the metadata for a single frame as `frame_meta`, cast to the class structure type `NvDsFrameMeta`. \n",
    "Each `frame_meta` includes an object metadata list, `l_obj`. \n",
    "\n",
    "\n",
    "```Python\n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta\n",
    "            # The casting is done by pyds.NvDsFrameMeta.cast()\n",
    "            # The casting also keeps ownership of the underlying memory\n",
    "            # in the C code, so the Python garbage collector will leave\n",
    "            # it alone.\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frame_number=frame_meta.frame_num\n",
    "        num_rects = frame_meta.num_obj_meta\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Count the Objects\n",
    "Another _while_ object loop identifies individual object metatdata within the object metadata list as `obj_meta` with structure `NvDsObjectMeta`.  At this point we have finally drilled down deep enough to count the objects!\n",
    "\n",
    "\n",
    "```Python\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                # Casting l_obj.data to pyds.NvDsObjectMeta\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            obj_counter[obj_meta.class_id] += 1\n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "```\n",
    "As shown in the metadata structure diagram, the `NvDsFrameMeta` structure includes a number of elements, including `class_id`. The object list _while_ loop in the probe uses the `obj_meta.class_id` value as an index to count the objects detected in each class. This is the same class number used in the config file to identify object types: \n",
    " - 0 for vehicles\n",
    " - 1 for bicycles\n",
    " - 2 for persons\n",
    " - 3 for road signs\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1.2 Display Frame Information through MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to extracting information from the metadata for analysis, you can also write data into the metadata structure to display results in the output stream.\n",
    "\n",
    "A `display_meta` object of type `NvDsDisplayMeta` is allocated to be copied later into `frame_meta`.  The text element of `display_meta` is set as the `py_nvosd_text_params` variable, of structure `NvOSD_TextParams`, with the following elements: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_nvosdtextparams.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, all four types of objects are always counted.  However, there will be a count of \"0\" if the object is not detected, as just seen in the \"Detect Only Two Object Types\" exercise in the previous notebook.  The `py_nvosd_text_params.display_text` buffer can be modified to change which counts are displayed. \n",
    "\n",
    "Other display properties such as font, font size, offset, and even color can be modified by changing additional `py_nvosd_text_params.font_params` of type `NvOSD_FontParams`.  The font parameter variables that can be changed are `font_name`, `font_size`, and `font_color`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "        # Acquiring a display meta object. The memory ownership remains in\n",
    "        # the C code so downstream plugins can still access it. Otherwise\n",
    "        # the garbage collector will claim it when this probe function exits.\n",
    "        display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta)\n",
    "        display_meta.num_labels = 1\n",
    "        py_nvosd_text_params = display_meta.text_params[0]\n",
    "        # Setting display text to be shown on screen\n",
    "        # Note that the pyds module allocates a buffer for the string, and the\n",
    "        # memory will not be claimed by the garbage collector.\n",
    "        # Reading the display_text field here will return the C address of the\n",
    "        # allocated string. Use pyds.get_string() to get the string content.\n",
    "        py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON])\n",
    "\n",
    "        # Now set the offsets where the string should appear\n",
    "        py_nvosd_text_params.x_offset = 10\n",
    "        py_nvosd_text_params.y_offset = 12\n",
    "\n",
    "        # Font , font-color and font-size\n",
    "        py_nvosd_text_params.font_params.font_name = \"Serif\"\n",
    "        py_nvosd_text_params.font_params.font_size = 10\n",
    "        # set(red, green, blue, alpha); set to White\n",
    "        py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "        # Text background color\n",
    "        py_nvosd_text_params.set_bg_clr = 1\n",
    "        # set(red, green, blue, alpha); set to Black\n",
    "        py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0)\n",
    "        # Using pyds.get_string() to get display_text as string\n",
    "        print(pyds.get_string(py_nvosd_text_params.display_text))\n",
    "        pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepStream Python API Reference at https://docs.nvidia.com/metropolis/deepstream/python-api/index.html provides details for all of the metadata structure properties and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1.3 Object Text Display\n",
    "Although not shown explicitly in the diagram, each `NvDsObjectMeta` object includes a `text_params` element, with the same structure as the one used for frames, `NvOSD_TextParams`.  If you want to change the size of your object fonts (the text next to the bounding boxes), you can add parameters inside the object _while_ loop. Here is an example of how the object loop previously shown could be changed to include an alternate font size:\n",
    "```Python\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                # Casting l_obj.data to pyds.NvDsObjectMeta\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            obj_counter[obj_meta.class_id] += 1\n",
    "            \n",
    "            # adjust text size for this object\n",
    "            obj_text_params = obj_meta.text_params\n",
    "            obj_text_params.font_params.font_name = \"Serif\"\n",
    "            obj_text_params.font_params.font_size = 20\n",
    "            \n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "```\n",
    "The DeepStream Python API Reference at https://docs.nvidia.com/metropolis/deepstream/python-api/index.html provides a complete and detailed reference for all of the metadata class structures, attributes, and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1.4 Exercise: Count Vehicles and Bikes\n",
    "Create a new app based on `deepstream-test1-rtsp_out` that displays counts for only vehicles and bicycles.  Begin by creating the project in `my_apps`, then modify the Python file.  Finally, run the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some path locations for readability\n",
    "PYTHON_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/apps'\n",
    "STREAMS = '/opt/nvidia/deepstream/deepstream/samples/streams'\n",
    "MY_APPS = '/opt/nvidia/deepstream/deepstream/sources/deepstream_python_apps/my_apps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new app located at my_apps/dst1-counts\n",
    "#      based on deepstream-test1-rtsp_out\n",
    "!mkdir -p $MY_APPS/dst1-counts\n",
    "!cp -rfv $PYTHON_APPS/deepstream-test1-rtsp-out/* $MY_APPS/dst1-counts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you just learned, modify the [dst1-counts/deepstream_test1_rtsp_out.py](my_apps/dst1-counts/deepstream_test1_rtsp_out.py) Python file in your new app to display the counts for vehicles and bicycles.  If the display text is difficult to see, you may also wish to increase the font size by modifying the `py_nvosd_text_params.font_params.font_size` value.<br>\n",
    "Then, run the app to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the app\n",
    "!cd $MY_APPS/dst1-counts \\\n",
    "    && python3 deepstream_test1_rtsp_out.py -i $STREAMS/sample_720p.h264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "If you see something like this image with only vehicle and bicycle counts displayed at the top, you did it!  If not, keep trying or take a peek at the [Python solution](solutions/ex2.1.4_CountTwo/ex2.1.4_deepstream_test1_rtsp_out.py).  Note that in this case we are still detecting all the objects, but only displaying counts for vehicles and bicycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_counts.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Put It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job!  In the previous notebook you learned how to detect specific objects, and now you've learned to probe/modify the metadata to count the objects.  It's time to put what you've learned about configuration and metadata into one new app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Exercise: Detect and Count Three Object Types\n",
    "Create a new app based on `deepstream-test1-rtsp_out` that detects and displays counts for _only_ three kinds of objects: persons, vehicles, and bicycles.  Adjust the confidence values if needed for each.  Fill in the following cells with appropriate commands to create and run your app.  You will need to modify both the config file and the Python file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Create a new app located at my_apps/dst1-three-things\n",
    "#      based on deepstream-test1-rtsp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Run the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did you do?\n",
    "If you see something like this image, you did it!  If not, keep trying or take a peek at the [solution](solutions/ex2.2.1_CountThree/solution-2.2.1.ipynb) in the solutions directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/01_three_things.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've created DeepStream apps to detect and count objects in a scene in various configurations.<br>\n",
    "Move on to [3.0 Multiple Networks Application](./03_MultipleNetworks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/DLI Header.png\" alt=\"Header\" width=\"400\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
